---
title: "SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns"
Author: "Feng, Yuye and Zhang, Wei and Fu, Yao and Jiang, Weihao and Zhu, Jiang and Ren, Wenqi."
Published: "KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
Date: 2025-09-04
Category: anomaly detection
---

# Introduction

기존의 재구성 기반 접근(autoencoder, VAE, Transformer 등)은 주로 reconstruction error를 최소화하는 방식이지만, 다음과 같은 한계를 가짐:

- **의존성 둔감성(Dependency-insensitive)**: 시간축과 채널 간 상관관계를 제대로 학습하지 않고도 데이터를 재구성해 버림.
- **분산 무시(Variance-agnostic)**: 채널마다 다른 잡음 수준(이분산)을 고려하지 않음.

<br/>

Dependency sensitive함과 이분산을 고려하는 loss 설계가 어떻게 이상탐지 성능을 크게 개선하는지 설명함.

## Dependency Sensitivity

기존 재구성 방식(압축 기반, 마스킹 기반)은 모델이 시·공간적 의존성을 반드시 학습하도록 강제하지 못함. 예를 들어:

- Temporal disruption: window에서 다른 시간 대의 값을 지워버려도 재구축이 잘됨
- Spatial disruption: 다른 channel의 데이터를 mix해도 재구축이 잘됨

<br/>

시·공간적 의존성이 잘 학습되었다면, 이러한 변형이 오면 성능이 많이 떨어져야 함. Toy experiment에서는 그런데 안떨어짐. 따라서 이 관계를 잘 학습할 수 있게 만들어야 함.

# Methodology

## Statistical Feature Removal (SFR)

논문은 이를 위해 통계적 특징 재구축(SFR; statistical feature reconstruction)을 제안함.

- 평균과 분산 제거

  각 input window 기준으로 채널마다 평균과 분산을 제거해 정규화 수행.

$$
x_{ts} = \frac{x_{ts}-\mu_{s}}{\sigma_{s}}
$$

$$
\mu_{s} = \frac{1}{T} \sum_{t=1}^{T} x_{ts}, \quad \sigma_{s}^2 = \frac{1}{T} \sum_{t=1}^{T} (x_{ts}-\mu_{s})^2
$$

모델은 제거된 통계량까지 포함해 원래 시퀀스를 복원하는 것을 목표로 학습됨. 이렇게 하면 단순히 통계량을 복사할 수 없고, 시간·채널 의존성을 반드시 활용해야만 정확한 복원이 가능해짐.

모델에게 통계량 복원까지 역할을 주었기 때문에, RevIN과는 다른 방법이 됨.

## MSE의 한계

$$
\mathcal{L}_{MSE} = \frac{1}{N}\sum (x - \hat{x})^2
$$

- 최적해는 분포의 조건부 평균, $\mu_{s}$으로 수렴.
- 예측 분산은 0으로 수축되어 불확실성을 반영하지 못함.
- 잡음이 큰 채널에도 동일한 비중을 주어, 본질적으로 예측 불가능한 영역에 학습 자원을 낭비하게 됨.

## Negative Log-Likelihood (NLL)

이를 보완하기 위해 확률적 관점을 도입함. 각 값이 정규분포를 따른다고 가정.

$$
x_{ts} \sim \mathcal{N}(\hat\mu_{ts}, \hat\sigma_{ts}^2)
$$

<br/>

손실 함수는 다음과 같음.

$$
 \mathcal{L}\_{NLL} (\hat{\mu\_{ts}}, \hat{\sigma\_{ts}}) = \frac{(x\_{ts} - \hat{\mu\_{ts}})^2}{2\hat{\sigma\_{ts}^2}} + \frac{1}{2}\ln \hat{\sigma\_{ts}^2} + const.
$$

- 첫 번째 항: 오차를 분산으로 스케일링 → 잡음이 큰 채널은 영향 ↓.
- 두 번째 항: 분산이 무한정 커지는 것 방지.
- 결과적으로 모델은 **평균과 분산**을 동시에 학습 → 이분산 불확실성을 반영.

## Case 1 vs. Case 2

문제가 되는 대표적인 두 상황 고려가 필요함

- **Case 1: 저 SNR, 고잡음 채널**

  - 긴 구간 동안 잡음이 심함.
  - 본질적으로 예측이 어렵고 학습을 방해.
  - 영향 축소될 필요가 있음.

- **Case 2: 고 SNR, 안정적 채널 + 짧은 이상 구간**
  - 평소에는 안정적.
  - 작은 이상도 정보량이 큼.
  - 민감도 강화될 필요가 있음.

## 왜 β-NLL은 Case 1에 부적합한가?

β-NLL은 NLL을 변형한 방식으로, 평균에 대한 그래디언트는 다음과 같음:

$$
\nabla_{\mu} L_{\beta\text{-NLL}} = \frac{( \mu - x )}{\hat\sigma^{2-2\beta}}
$$

- **저분산 채널**: 기울기가 커져서 이상에 민감 → Case 2에는 유리.
- **고분산 채널**: 상대 가중이 커져서 오히려 학습이 잡음에 끌림 → Case 1에는 불리.

즉, β-NLL은 잡음 채널을 과대평가하는 **편향**이 존재함.

## MTS-NLL: 채널 단위 정규화

이를 해결하기 위해 **MTS-NLL**이 제안됨. 채널별 평균 분산을 계산해 정규화 계수로 사용함.

$$
\hat\sigma^2_{s,\text{mean}} = \frac{1}{BT} \sum_{b=1}^B \sum_{t=1}^T \hat\sigma^2_{bts}
$$

<br/>

그리고 각 채널 손실을 다음처럼 재가중됨.

$$
\Big\lfloor \frac{1}{\hat\sigma^{2\alpha}_{s,\text{mean}}} \Big\rfloor
$$

- **고잡음 채널 (Case 1)** → 평균 분산 ↑ → 가중 ↓.
- **안정적 채널 (Case 2)** → 평균 분산 ↓ → 가중 ↑.

<br/>

결과적으로:

- Case 1: 잡음 구간 영향 최소화.
- Case 2: 이상 탐지 민감도 향상.

## 전체 요약

| Model              | SWaT P    | SWaT R    | SWaT F1\* | SWaT F1\*PA | WADI P    | WADI R    | WADI F1\* | WADI F1\*PA | MSL P     | MSL R     | MSL F1\*  | MSL F1\*PA | SMD P     | SMD R     | SMD F1\*  | SMD F1\*PA |
| ------------------ | --------- | --------- | --------- | ----------- | --------- | --------- | --------- | ----------- | --------- | --------- | --------- | ---------- | --------- | --------- | --------- | ---------- |
| IF                 | 91.32     | 63.02     | 74.57     | 86.19       | 24.59     | 38.60     | 30.04     | 69.38       | 19.92     | 71.17     | 31.18     | 83.31      | 22.40     | 53.04     | 31.50     | 89.62      |
| DAGMM              | 27.46     | 69.52     | 39.37     | 85.33       | 54.44     | 26.99     | 36.09     | 61.65       | 25.91     | 62.86     | 36.69     | 70.09      | 18.29     | 50.46     | 26.85     | 72.29      |
| NSIBF              | 96.21     | 65.27     | 77.19     | 90.50       | 7.13      | 84.76     | 13.16     | 23.64       | 20.19     | 63.02     | 30.58     | 63.82      | 36.86     | 58.67     | 45.27     | 81.82      |
| GDN                | 99.35     | 68.12     | 80.62     | 93.50       | 97.50     | 50.19     | 66.29     | 85.82       | 21.33     | 74.11     | 33.14     | 90.30      | 44.70     | 74.68     | 56.90     | 71.14      |
| LSTM-VAE           | 96.24     | 59.91     | 73.85     | 90.48       | 87.79     | 14.45     | 24.28     | 35.82       | 18.83     | 70.58     | 36.66     | 67.77      | 26.89     | 58.05     | 36.90     | 71.83      |
| OmniAnomaly        | 98.25     | 64.97     | 78.22     | 91.61       | 99.47     | 12.96     | 22.96     | 41.72       | 16.19     | 84.06     | 27.18     | 89.94      | 20.61     | 46.73     | 28.73     | 94.43      |
| USAD               | 98.51     | 66.18     | 79.17     | 84.48       | 95.38     | 13.73     | 23.28     | 42.96       | 21.67     | 74.51     | 33.91     | 92.72      | 21.57     | 55.53     | 31.07     | 94.63      |
| TranAD             | 94.86     | 61.49     | 74.61     | 81.51       | 88.76     | 15.50     | 26.39     | 49.51       | 29.06     | 75.96     | 42.04     | 94.94      | 30.27     | 53.43     | 38.65     | 95.05      |
| AnomalyTran        | 12.00     | 100.00    | 21.43     | 94.07       | 5.79      | 43.43     | 10.21     | 89.10       | -         | -         | 2.10      | 93.59      | -         | -         | 2.12      | 92.33      |
| D³R                | 12.04     | 99.59     | 21.49     | 90.55       | 6.32      | 83.19     | 11.75     | 35.46       | 11.04     | 93.01     | 19.74     | 87.44      | 23.70     | 52.63     | 32.62     | 95.09      |
| PUAD               | 97.94     | 62.50     | 75.71     | 82.74       | 95.51     | 15.00     | 25.92     | 41.25       | 25.85     | 75.03     | 38.45     | 95.04      | 26.95     | 56.27     | 36.74     | 96.16      |
| NPSR               | 93.42     | 75.52     | 83.52     | 91.07       | 78.43     | 50.33     | 61.31     | 75.22       | 24.03     | 83.92     | 37.37     | 93.20      | 25.58     | 62.36     | 37.27     | 76.95      |
| **SensitivityHUE** | **94.68** | **87.74** | **91.08** | **96.75**   | **86.51** | **58.73** | **69.96** | **92.25**   | **33.05** | **71.26** | **45.16** | **98.42**  | **29.54** | **60.80** | **39.76** | **96.33**  |

- **SFR**: 모델이 반드시 시·공간적 의존성을 활용하도록 강제.
- **MTS-NLL**: 채널별 잡음 수준을 반영해 이분산 문제 해결.
- 두 전략을 결합해 SWaT, WADI 데이터셋에서 F1 점수를 크게 향상.

## 핵심 포인트

1. **아키텍처만 잘 짜는 것으로는 부족하다** → 의존성 민감성을 강제해야 한다.
2. **MSE는 단순하다** → 분산과 불확실성을 무시한다.
3. **NLL은 MSE보다 낫다** → 평균과 분산을 모두 모델링. 그러나 β-NLL은 고잡음 케이스를 망친다.
4. **MTS-NLL이 해답** → 채널 단위 정규화를 통해 Case 1과 Case 2 모두 균형 잡힌 대응.

# Conclusion

다변량 시계열 이상탐지에서 중요한 것은 단순히 강력한 모델을 쓰는 것이 아니다.  
**의존성 민감성과 이분산 인식**을 설계에 반영할 때, 모델은 잡음에 더 강하고 실제 이상에 더 민감해진다.
